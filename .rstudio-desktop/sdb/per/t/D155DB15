{
    "collab_server" : "",
    "contents" : "# ANALYSIS\n## Observations, remarks, model documentation etc.\n\n### 1. Basic statistics\n\nAssumption it's an eCommerce business based on the transactions.\nRight after the first moment it was obvious there is a decreasing trend so there is a room for improvement and the returning customers are valueable.\n\n\nAll the transactions are really skewed considering the amount and quantity also. This is really typical to eCommerce and we have to handle all the outliers to build a reliable model to predict any CLV related metric.\n\n### 2. High overview predictions\n\nIt's obvious the upcoming trend is slightly decreasing so the expected monetary values probably will be lower.\nI've created a TBATS model to forecast the sales amount numbers to have a high level estimate.\n\n![alt tag](https://gitlab.metricbrew.com/tamas.szuromi/emarsys-takehome-challenge/raw/master/plots/fc-monetary.png)\n\n# 1,087,639\n\nThe forecasted sales amount for the upcoming year. For 95% conf there is a range between 19,813,138 and 10,087,812.\n\n### 3. Cohorts\n\nFrom the cohorts I observed a poor retention rate. Also the new visitors are decreasing so the outlook is not really great.\n\n\n### 4. Forecast sales amounts\n\nFirst I tried to fit churn model to predict the retention rate what was observed in cohorts. At a first glance any rf, gbm or deep models feature building seemed really hard and I had a feeling this is not the right way so I thought I need a probabilistic model to get the best results.\n\nI was really impressed by Roberto Medri (Etsy) talk so I tried to move this way.  \nhttp://cdn.oreillystatic.com/en/assets/1/event/85/Case%20Study_%20What_s%20a%20Customer%20Worth_%20Presentation.pdf\n\nThe most important take away from the talk what could be used here is the following:\nAt every moment, customer flips two coins:  \nThe first coin determines if the customer lives or dies  \n(e.g. forgets about your business).   \nThe second coin determines if she buys or not.   \n\nThere are many models out there to use to this problem.    \nFor the live/die problem there is the Buy Til You Die library in R and the https://github.com/CamDavidsonPilon/lifetimes for Python.  \n\nAfter few quick tests with the MBG/NBD model was the best performing on the training test. The python implementation was a bit different and much easier to use so I moved on with the python package.   \n\nThe model's implementation with commentary can be found in the CLV notebook.   \n\nThe final model could be tested in the modelPred notebook with the given function easily.   \n\n### 5. Model's summary\n\nBased on the customers historical frequency and recency of buys with the combination of the customers age the future transactions are predictable. Also if we put the sales amount to the model we can estimate the average sales amount of transactions. Putting all together all the future sales amounts are predictable.\n\nBG/NBD model   \nIn addition to the Pareto/NBD based models it's handling better the non returning customers in the training period. This is really needed to estimate future transactions of customers with recent first buy. The given dataset has a lot of these so it's handy.\nThe model's based on the followings:\n* While active, the number of transactions made by a customer follows a Poisson process\nwith transaction rate λ. This is equivalent to assuming that the time between transactions\nis distributed exponential with transaction rate λ.\n*  Heterogeneity in λ follows a gamma distribution\n*  After any transaction, a customer becomes inactive with probability p. Therefore the\npoint at which the customer “drops out” is distributed across transactions according to a\n(shifted) geometric distribution\n* Heterogeneity in p follows a beta distribution\n* The transaction rate λ and the dropout probability p vary independently across customers\n* Based on numerical optimization we can estimate the parameters\n\n\n\n### 6. Technical part\n\n#### R part\n\nIn the R/init.R you can check the required packages and assets for the R/exploration.R. It's also install/update the required packages.   \n\nThe exploration.R is just a dump of the exploratory analysis part.\n\n#### Python part\n\nFor the python part anaconda installation is recommended because it has most of the packages.   \nIf needed you can download it here: https://www.continuum.io/downloads  \n\nMaybe you need some extra packages to install what could be need to run the notebooks. So with pip install -r python/requirements.txt you can install/update the requirements.   \n\nThe best way to check the files is to use jupyter notebooks. In a command line just hit jupyter notebook and navigate to the python dir in the repository.\n",
    "created" : 1484125569803.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3132388058",
    "id" : "D155DB15",
    "lastKnownWriteTime" : 1483441368,
    "last_content_update" : 1483441368,
    "path" : "~/homeworks/Szuromi/ANALYSIS.md",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "markdown"
}